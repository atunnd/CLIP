{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Vector Generation using FiftyOne and Pinecone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates text embeddings for the COCO2017 dataset, using CLIP. These are both loaded from FiftyOne. Execute each cell, and create a config.py file in your directory, in which you should place your PINECONE_KEY. Make sure you have a Pinecone Index available to create a new one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from pkg_resources import packaging\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "#from config import PINECONE_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\Dell\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\Dell\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n",
    "model = foz.load_zoo_model(\"clip-vit-base32-torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if packaging.version.parse(\n",
    "  torch.__version__\n",
    ") < packaging.version.parse(\"1.8.0\"):\n",
    "  dtype = torch.long\n",
    "else:\n",
    "  dtype = torch.int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5000/5000 [15.4m elapsed, 0s remaining, 6.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# generating the embeddings\n",
    "dataset.compute_embeddings(\n",
    "    model, \n",
    "    embeddings_field=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the data in my computer\n",
    "dataset.persistent = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the pinecone index and upserting the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone client\n",
    "#pinecone.init(api_key='508a1fea-8fd8-4b51-ae51-053df59dd9a7', environment=\"us-east4-gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "pc = Pinecone(api_key=\"508a1fea-8fd8-4b51-ae51-053df59dd9a7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc.create_index(\n",
    "    \"clip-image-search\", \n",
    "    dimension=512, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")\n",
    "# initialize index\n",
    "index = pc.Index(\"clip-image-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy arrays to lists for pinecone\n",
    "embeddings = [arr.tolist() for arr in dataset.values(\"embedding\")]\n",
    "#ids = [\"http://images.cocodataset.org/val2017/\" + file.split('/')[-1] for file in dataset.values(\"filepath\")]\n",
    "ids = [file for file in dataset.values(\"filepath\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuples of (id, embedding) for each sample\n",
    "index_vectors = list(zip(ids, embeddings))\n",
    "\n",
    "# upsert vectors in batches of 100\n",
    "def upsert_vectors(index, vectors):\n",
    "    num_vectors = len(vectors)\n",
    "    num_vectors_per_step = 100\n",
    "    num_steps = int(np.ceil(num_vectors/num_vectors_per_step))\n",
    "    for i in range(num_steps):\n",
    "        min_ind = num_vectors_per_step * i\n",
    "        max_ind = min(num_vectors_per_step * (i+1), num_vectors)\n",
    "        index.upsert(index_vectors[min_ind:max_ind])\n",
    "\n",
    "upsert_vectors(index, index_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(prompt, clip_model):\n",
    "    tokenizer = clip_model._tokenizer\n",
    "\n",
    "    # standard start-of-text token\n",
    "    sot_token = tokenizer.encoder[\"<|startoftext|>\"]\n",
    "\n",
    "    # standard end-of-text token\n",
    "    eot_token = tokenizer.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    all_tokens = [[sot_token] + prompt_tokens + [eot_token]]\n",
    "\n",
    "    text_features = torch.zeros(\n",
    "        len(all_tokens),\n",
    "        clip_model.config.context_length,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # insert tokens into feature vector\n",
    "    text_features[0, : len(all_tokens[0])] = torch.tensor(all_tokens)\n",
    "\n",
    "    # encode text\n",
    "    embedding = clip_model._model.encode_text(text_features).to(device)\n",
    "\n",
    "    # convert to list for Pinecone\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mquery_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000019924.jpg',\n",
       "              'score': 0.257860392,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000167067.jpg',\n",
       "              'score': 0.240282223,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000139260.jpg',\n",
       "              'score': 0.237064257,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000474854.jpg',\n",
       "              'score': 0.235772446,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000022705.jpg',\n",
       "              'score': 0.235523269,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000402096.jpg',\n",
       "              'score': 0.235292241,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000389812.jpg',\n",
       "              'score': 0.234833553,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000007888.jpg',\n",
       "              'score': 0.234058082,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000301981.jpg',\n",
       "              'score': 0.233422041,\n",
       "              'values': []},\n",
       "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000261712.jpg',\n",
       "              'score': 0.231851265,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"a smile\"\n",
    "query_vector = get_text_embedding(prompt, model)\n",
    "top_k_samples = index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=10,\n",
    "    include_values=False\n",
    ")\n",
    "\n",
    "top_k_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "import skimage\n",
    "import requests\n",
    "import pinecone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_URL):\n",
    "   response = requests.get(image_URL)\n",
    "   image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_ID, device):\n",
    "\t# Save the model to device\n",
    "\tmodel = CLIPModel.from_pretrained(model_ID).to(device)\n",
    " \t# Get the processor\n",
    "\tprocessor = CLIPProcessor.from_pretrained(model_ID)\n",
    "\t# Get the tokenizer\n",
    "\ttokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "  # Return model, processor & tokenizer\n",
    "\treturn model, processor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pinecone\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Define the model ID\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "# Get model, processor & tokenizer\n",
    "model, processor, tokenizer = get_model_info(model_ID, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIPModel(\n",
       "   (text_model): CLIPTextTransformer(\n",
       "     (embeddings): CLIPTextEmbeddings(\n",
       "       (token_embedding): Embedding(49408, 512)\n",
       "       (position_embedding): Embedding(77, 512)\n",
       "     )\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPAttention(\n",
       "             (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (vision_model): CLIPVisionTransformer(\n",
       "     (embeddings): CLIPVisionEmbeddings(\n",
       "       (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "       (position_embedding): Embedding(50, 768)\n",
       "     )\n",
       "     (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "   (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       " ),\n",
       " CLIPProcessor:\n",
       " - image_processor: CLIPImageProcessor {\n",
       "   \"_valid_processor_keys\": [\n",
       "     \"images\",\n",
       "     \"do_resize\",\n",
       "     \"size\",\n",
       "     \"resample\",\n",
       "     \"do_center_crop\",\n",
       "     \"crop_size\",\n",
       "     \"do_rescale\",\n",
       "     \"rescale_factor\",\n",
       "     \"do_normalize\",\n",
       "     \"image_mean\",\n",
       "     \"image_std\",\n",
       "     \"do_convert_rgb\",\n",
       "     \"return_tensors\",\n",
       "     \"data_format\",\n",
       "     \"input_data_format\"\n",
       "   ],\n",
       "   \"crop_size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   },\n",
       "   \"do_center_crop\": true,\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"shortest_edge\": 224\n",
       "   }\n",
       " }\n",
       " \n",
       " - tokenizer: CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"CLIPProcessor\"\n",
       " },\n",
       " CLIPTokenizer(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, processor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_URL):\n",
    "   response = requests.get(image_URL)\n",
    "   image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/640px-Cat03.jpg'\n",
    "image_test = get_image(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_embedding(my_image,processor, model, device):\n",
    "  image = processor(\n",
    "      text = None,\n",
    "      images = my_image,\n",
    "      return_tensors=\"pt\"\n",
    "      )[\"pixel_values\"].to(device)\n",
    "  embedding = model.get_image_features(image)\n",
    "  # convert the embeddings to numpy array\n",
    "  return embedding.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector_embeddings = get_single_image_embedding(image_test, processor, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector_embeddings = image_vector_embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000209747.jpg',\n",
      "              'score': 0.796100795,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000213445.jpg',\n",
      "              'score': 0.786400259,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000491757.jpg',\n",
      "              'score': 0.777820408,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000524280.jpg',\n",
      "              'score': 0.775030613,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000574810.jpg',\n",
      "              'score': 0.772797406,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000291490.jpg',\n",
      "              'score': 0.770050347,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000155291.jpg',\n",
      "              'score': 0.7683025,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000245576.jpg',\n",
      "              'score': 0.766795576,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000077595.jpg',\n",
      "              'score': 0.760326147,\n",
      "              'values': []},\n",
      "             {'id': 'C:\\\\Users\\\\Dell\\\\fiftyone\\\\coco-2017\\\\validation\\\\data\\\\000000411665.jpg',\n",
      "              'score': 0.759701431,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 5}}\n"
     ]
    }
   ],
   "source": [
    "top_k_samples = index.query(\n",
    "    vector=image_vector_embeddings,\n",
    "    top_k=10,\n",
    "    include_values=False\n",
    ")\n",
    "print(top_k_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
